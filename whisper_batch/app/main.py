from __future__ import annotations

import datetime
import os
import shutil
import sys
import time
import traceback
import io
import pandas as pd
import json
from pathlib import Path
from typing import Any, Dict, List, Optional

from dotenv import load_dotenv
from google.cloud import firestore, storage
from common_utils.class_types import WhisperFirestoreData
from common_utils.logger import logger

# â”€â”€ å¤–éƒ¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# convert_audio ã¨ check_audio_format ã¯å¿…è¦ãªããªã‚Šã¾ã—ãŸ
from whisper_batch.app.transcribe import transcribe_audio
from whisper_batch.app.diarize import diarize_audio
from whisper_batch.app.combine_results import (
    combine_results,
    read_json,
    save_dataframe,
)

# â”€â”€ .env èª­ã¿è¾¼ã¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR = Path(__file__).resolve().parent.parent
config_path = os.path.join(BASE_DIR, "config", ".env")
load_dotenv(config_path)

develop_config_path = os.path.join(BASE_DIR, "config_develop", ".env.develop")
if os.path.exists(develop_config_path):
    load_dotenv(develop_config_path)

if str(BASE_DIR) not in os.environ.get("GOOGLE_APPLICATION_CREDENTIALS", ""):
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.join(
        BASE_DIR, os.environ.get("GOOGLE_APPLICATION_CREDENTIALS", "")
    )

# Environment variables passed from GCP Batch
FULL_AUDIO_GCS_PATH_ENV = "FULL_AUDIO_PATH"
FULL_TRANSCRIPTION_GCS_PATH_ENV = "FULL_TRANSCRIPTION_PATH"

# â”€â”€ ç’°å¢ƒå¤‰æ•°ï¼ˆæœªè¨­å®šæ™‚ã¯ KeyError ã‚’ç™ºç”Ÿã•ã›ã‚‹ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    COLLECTION = os.environ['COLLECTION']
except KeyError:
    raise RuntimeError(
        'Environment variable COLLECTION is required for whisper_batch'
    )
# PROCESS_TIMEOUT_SECONDS ã¨ AUDIO_TIMEOUT_MULTIPLIER ã®èª­ã¿è¾¼ã¿ã¯ backend å´ã«ç§»ç®¡ã™ã‚‹ãŸã‚å‰Šé™¤
# PROCESS_TIMEOUT_SECONDS ã¨ AUDIO_TIMEOUT_MULTIPLIER ã®èª­ã¿è¾¼ã¿ã¯ backend å´ã«ç§»ç®¡ã™ã‚‹ãŸã‚å‰Šé™¤
POLL_INTERVAL_SECONDS: int = int(
    os.environ.get("POLL_INTERVAL_SECONDS", "10") # Default if not set
)
HF_AUTH_TOKEN: str = os.environ["HF_AUTH_TOKEN"]
DEVICE: str = os.environ["DEVICE"].lower()
USE_GPU: bool = DEVICE == "cuda"
TMP_ROOT: Path = Path(os.environ["LOCAL_TMP_DIR"])
batch_bucket = os.environ["GCS_BUCKET"]

AUDIO_TEMPLATE = os.environ["WHISPER_AUDIO_BLOB"]
TRANS_TEMPLATE = os.environ["WHISPER_TRANSCRIPT_BLOB"]
DIAR_TEMPLATE = os.environ["WHISPER_DIARIZATION_BLOB"]
COMBINE_TEMPLATE = os.environ["WHISPER_COMBINE_BLOB"]


def _utcnow() -> datetime.datetime:
    return datetime.datetime.now(tz=datetime.timezone.utc)

# _mark_timeout_jobs é–¢æ•°ã‚’å‰Šé™¤ã—ã¾ã™

def _pick_next_job(db: firestore.Client) -> Optional[Dict[str, Any]]:
    @firestore.transactional
    def _txn(tx: firestore.Transaction) -> Optional[Dict[str, Any]]:
        col = db.collection(COLLECTION)
        docs = (
            col.where("status", "in", ["queued", "launched"]) # queued ã¾ãŸã¯ launched ã‚’å¯¾è±¡
            .order_by("upload_at")
            .limit(1)
            .stream(transaction=tx)
        )
        docs = list(docs)
        if not docs:
            return None
        doc = docs[0]
        tx.update(
            doc.reference,
            {
                "status": "processing",
                "process_started_at": firestore.SERVER_TIMESTAMP,
                "updated_at": firestore.SERVER_TIMESTAMP,
            },
        )
        try:
            data = doc.to_dict()
            data["job_id"] = doc.id
            data["status"] = "processing" # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
            firestore_data = WhisperFirestoreData(**data)
            return dict(firestore_data.model_dump())
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ (job_id={doc.id}): {e}")
            # ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å†…ã§ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã¦ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã•ã›ã‚‹ã‹ã€
            # ã‚ã‚‹ã„ã¯ã“ã®ã‚¸ãƒ§ãƒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆfailedã«ã™ã‚‹ãªã©ï¼‰ã™ã‚‹ã‹æ¤œè¨ãŒå¿…è¦
            # ã“ã“ã§ã¯Noneã‚’è¿”ã—ã¦å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹
            return None
    return _txn(db.transaction())


def _process_job(db: firestore.Client, job: Dict[str, Any]) -> None:
    # WhisperFirestoreDataã§ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚’è©¦ã¿ã‚‹
    try:
        firestore_data = WhisperFirestoreData(**job)
        job_id = firestore_data.job_id
        filename = firestore_data.filename
        bucket = firestore_data.gcs_bucket_name
        file_hash = firestore_data.file_hash
        # language, initial_prompt, num_speakers, min_speakers, max_speakers ã‚’å–å¾—
        language = firestore_data.language
        initial_prompt = firestore_data.initial_prompt
        num_speakers = firestore_data.num_speakers
        min_speakers = firestore_data.min_speakers
        max_speakers = firestore_data.max_speakers

    except Exception as e:
        job_id = job.get("job_id", "<unknown>")
        msg = f"ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}"
        logger.error(f"JOB {job_id} âœ– {msg}")
        if job_id != "<unknown>":
            db.collection(COLLECTION).document(job_id).update(
                {
                    "status": "failed",
                    "error_message": msg,
                    "updated_at": firestore.SERVER_TIMESTAMP,
                }
            )
        return

    ext = Path(filename).suffix.lstrip(".").lower()
    
    # Get full GCS paths from environment variables set by the batch job submission
    full_audio_gcs_path = os.environ[FULL_AUDIO_GCS_PATH_ENV]
    full_transcription_gcs_path = os.environ[FULL_TRANSCRIPTION_GCS_PATH_ENV]

    # Parse bucket and blob name from the full GCS path
    def parse_gcs_path(gcs_path: str) -> tuple[str, str]:
        if not gcs_path.startswith("gs://"):
            raise ValueError(f"Invalid GCS path: {gcs_path}")
        parts = gcs_path[5:].split("/", 1)
        if len(parts) != 2:
            raise ValueError(f"Cannot parse bucket and blob from GCS path: {gcs_path}")
        return parts[0], parts[1]

    audio_bucket_name, audio_blob_name = parse_gcs_path(full_audio_gcs_path)
    transcription_bucket_name, transcription_blob_name = parse_gcs_path(full_transcription_gcs_path)

    logger.info(f"JOB {job_id} â–¶ Start (audio: {full_audio_gcs_path})")

    tmp_dir = TMP_ROOT / f"job_{job_id}_{int(time.time())}"
    tmp_dir.mkdir(parents=True, exist_ok=True)
    storage_client = storage.Client()

    try:
        local_audio_filename = Path(audio_blob_name).name # Use the blob name for the local file
        local_audio = tmp_dir / local_audio_filename
        storage_client.bucket(audio_bucket_name).blob(audio_blob_name).download_to_filename(local_audio)
        logger.info(f"JOB {job_id} â¤µ Downloaded â†’ {local_audio} from {full_audio_gcs_path}")

        # éŸ³å£°ã¯ã™ã§ã«16kHzãƒ¢ãƒãƒ©ãƒ«WAVå½¢å¼ã«ãªã£ã¦ã„ã‚‹ã¯ãšãªã®ã§ã€å¤‰æ›ã¯ä¸è¦
        wav_path = local_audio
        logger.info(f"JOB {job_id} ğŸ§ ã™ã§ã«å¤‰æ›æ¸ˆã¿ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ â†’ {wav_path}")

        # Define local paths for intermediate files
        transcript_local_filename = f"{file_hash}_transcription.json" # Consistent naming
        transcript_local = tmp_dir / transcript_local_filename
        transcribe_audio(
            str(wav_path), str(transcript_local),
            device=DEVICE, job_id=job_id,
            language=language, initial_prompt=initial_prompt
        )
        logger.info(f"JOB {job_id} âœ Transcribed â†’ {transcript_local}")

        diarization_local_filename = f"{file_hash}_diarization.json" # Consistent naming
        diarization_local = tmp_dir / diarization_local_filename

        is_single_speaker = num_speakers == 1 or (
            num_speakers is None and max_speakers == 1 and min_speakers == 1 # min_speakersã‚‚è€ƒæ…®
        )

        if is_single_speaker:
            create_single_speaker_json(str(transcript_local), str(diarization_local))
            logger.info(f"JOB {job_id} ğŸ‘¤ Single speaker mode â†’ {diarization_local}")
        else:
            diarize_audio(
                str(wav_path),
                str(diarization_local),
                hf_auth_token=HF_AUTH_TOKEN,
                num_speakers=num_speakers,
                min_speakers=min_speakers,
                max_speakers=max_speakers,
                device=DEVICE,
                job_id=job_id,
            )
            logger.info(f"JOB {job_id} ğŸ‘¥ Diarized â†’ {diarization_local}")

        # The final combined output will be uploaded to the path specified by full_transcription_gcs_path
        # So the local filename for the combined result should match the blob name part of full_transcription_gcs_path
        combine_local_filename = Path(transcription_blob_name).name
        combine_local = tmp_dir / combine_local_filename
        
        # combine_results ã«æ¸¡ã™ãƒ‘ã‚¹ã‚’ä¿®æ­£
        combine_results(str(transcript_local), str(diarization_local), str(combine_local))
        logger.info(f"JOB {job_id} ğŸ”— Combined â†’ {combine_local}")

        # Upload the combined result
        storage_client.bucket(transcription_bucket_name).blob(transcription_blob_name).upload_from_filename(
            combine_local
        )
        logger.info(f"JOB {job_id} â¬† Uploaded combined result â†’ {full_transcription_gcs_path}")

        # å€‹åˆ¥ã®æ–‡å­—èµ·ã“ã—çµæœã¨è©±è€…åˆ†é›¢çµæœã‚‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆå¿…è¦ã§ã‚ã‚Œã°ï¼‰
        # storage_client.bucket(bucket).blob(transcript_blob).upload_from_filename(
        #     transcript_local
        # )
        # logger.info(f"JOB {job_id} â¬† Uploaded transcription â†’ {transcript_uri}")
        # storage_client.bucket(bucket).blob(diarization_blob).upload_from_filename(diarization_local)
        # logger.info(f"JOB {job_id} â¬† Uploaded diarization result â†’ {diarization_uri}")


        job_doc = db.collection(COLLECTION).document(job_id).get()
        if not job_doc.exists:
            logger.error(
                f"JOB {job_id} âœ– ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
            )
        else:
            current_status = job_doc.to_dict().get("status", "")
            if current_status not in ["processing", "launched"]: # backendå´ã§ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚ŒãŸå ´åˆãªã©ã‚’è€ƒæ…®
                logger.warning(
                    f"JOB {job_id} Current status is '{current_status}', not 'processing' or 'launched'. Skipping 'completed' update from worker."
                )
            else:
                # å‡¦ç†æˆåŠŸã€‚Firestoreã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®ã¿æ›´æ–°ï¼ˆsegmentsã¯ä¿å­˜ã—ãªã„ï¼‰
                db.collection(COLLECTION).document(job_id).update(
                    {
                        "status": "completed",
                        "process_ended_at": firestore.SERVER_TIMESTAMP,
                        "updated_at": firestore.SERVER_TIMESTAMP,
                        "error_message": None # æˆåŠŸæ™‚ã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚¯ãƒªã‚¢
                    }
                )
                logger.info(f"JOB {job_id} âœ” Completed.")

    except Exception as e:
        err = str(e)
        logger.error(f"JOB {job_id} âœ– Failed: {err}\n{traceback.format_exc()}")
        job_doc = db.collection(COLLECTION).document(job_id).get()
        if not job_doc.exists:
            logger.error(
                f"JOB {job_id} âœ– ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
            )
        else:
            # current_status = job_doc.to_dict().get("status", "") # backendå´ã§ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚ŒãŸå ´åˆãªã©ã‚’è€ƒæ…®
            # if current_status not in ["processing", "launched"]:
            #    logger.warning(f"JOB {job_id} Current status is '{current_status}', not 'processing' or 'launched'. Skipping 'failed' update from worker.")
            # else:
            db.collection(COLLECTION).document(job_id).update(
                {
                    "status": "failed",
                    "error_message": err,
                    "process_ended_at": firestore.SERVER_TIMESTAMP,
                    "updated_at": firestore.SERVER_TIMESTAMP,
                }
            )
    finally:
        shutil.rmtree(tmp_dir, ignore_errors=True)


def create_single_speaker_json(transcription_path_str: str, output_path_str: str):
    transcription_path = Path(transcription_path_str)
    output_path = Path(output_path_str)
    try:
        transcription_df = read_json(transcription_path)
        speaker_data = []
        for _, row in transcription_df.iterrows():
            speaker_data.append(
                {
                    "start": row["start"],
                    "end": row["end"],
                    "speaker": "SPEAKER_01",
                }
            )
        speaker_df = pd.DataFrame(speaker_data)
        # output_pathã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        output_path.parent.mkdir(parents=True, exist_ok=True)
        save_dataframe(speaker_df, output_path)
        logger.info(f"å˜ä¸€è©±è€…JSONã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {output_path}")
    except Exception as e:
        logger.error(f"å˜ä¸€è©±è€…JSONã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w") as f:
            json.dump([], f)


def main_loop() -> None:
    db = firestore.Client()
    logger.info("ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•ã—ã¾ã—ãŸã€‚ã‚¸ãƒ§ãƒ–ã‚­ãƒ¥ãƒ¼ã‚’ãƒãƒ¼ãƒªãƒ³ã‚°ã—ã¾ã™...")
    while True:
        try:
            job = _pick_next_job(db)
            if job:
                _process_job(db, job)
            else:
                # logger.info("ã‚­ãƒ¥ãƒ¼ãŒç©ºã§ã™ã€‚å¾…æ©Ÿâ€¦") # ãƒ­ã‚°å‡ºåŠ›ã‚’æŠ‘åˆ¶ã™ã‚‹å ´åˆ
                time.sleep(POLL_INTERVAL_SECONDS)
        except KeyboardInterrupt:
            logger.info("SIGINT å—ä¿¡ã€‚ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’çµ‚äº†ã—ã¾ã™")
            break
        except Exception as e:
            logger.error(f"Main loop error: {e}\n{traceback.format_exc()}")
            time.sleep(POLL_INTERVAL_SECONDS)

if __name__ == "__main__":
    # å¸¸ã«ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ¢ãƒ¼ãƒ‰ (main_loop) ã‚’å®Ÿè¡Œ
    main_loop()
